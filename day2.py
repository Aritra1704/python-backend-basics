# -*- coding: utf-8 -*-
"""Day2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xv0tBk3jx4kOXtUQqr8Mn-rdB0diBd8p

# Day 2: Backend Metrics & Log Analysis with Pandas

Goal:
- Analyze backend metrics like latency & errors
- Identify slow endpoints
- Build production-style insights
"""

import pandas as pd

logs = [
    {"endpoint": "/login", "latency_ms": 120, "status": "SUCCESS"},
    {"endpoint": "/login", "latency_ms": 980, "status": "FAIL"},
    {"endpoint": "/order", "latency_ms": 450, "status": "SUCCESS"},
    {"endpoint": "/order", "latency_ms": 780, "status": "SUCCESS"},
    {"endpoint": "/payment", "latency_ms": 1500, "status": "FAIL"},
    {"endpoint": "/payment", "latency_ms": 1300, "status": "FAIL"}
]

df = pd.DataFrame(logs)
df

df.describe()

error_rate = (
    df.assign(is_fail=df["status"] == "FAIL")
      .groupby("endpoint")["is_fail"]
      .mean()
)

error_rate

slow_endpoints = (
    df.groupby("endpoint")["latency_ms"]
      .mean()
      .sort_values(ascending=False)
)

slow_endpoints

SLO_MS = 500

slo_violations = df[df["latency_ms"] > SLO_MS]
slo_violations

report = {
    "avg_latency": df["latency_ms"].mean(),
    "max_latency": df["latency_ms"].max(),
    "error_rate": (df["status"] == "FAIL").mean(),
    "slowest_endpoint": slow_endpoints.index[0]
}

report

with open("day2_backend_metrics_report.txt", "w") as f:
    for k, v in report.items():
        f.write(f"{k}: {v}\n")

"""Key takeaways:
- Pandas is powerful for backend observability
- Faster than SQL for ad-hoc analysis
- Ideal for incident analysis and AI pipelines
"""

df.groupby("status")["latency_ms"].mean()

"""Backend insight
If failures are much slower â†’ timeout / retries
If failures are fast â†’ validation / auth issues
This directly informs:
Retry policy
Circuit breaker thresholds
"""

df["endpoint"].value_counts(normalize=True) * 100

"""Backend meaning
Shows traffic distribution
Helps prioritize optimization
High traffic + slow = ðŸ”¥ critical
"""

df["is_slow"] = df["latency_ms"] > 500
df

"""Backend meaning
Converts raw latency â†’ signal
Same idea as:
is_timeout
is_retry
is_cache_miss
"""

slo_breach_rate = (
    df.groupby("endpoint")["is_slow"]
      .mean()
      .sort_values(ascending=False)
)

slo_breach_rate

"""Lead-level interpretation
/payment is a clear risk
Even if traffic is low, impact is high
"""

p95 = df["latency_ms"].quantile(0.95)
outliers = df[df["latency_ms"] > p95]
outliers

"""Backend meaning
Spike analysis
Used in:
Alert tuning
Postmortems
Noise reduction
"""

endpoint_risk = (
    df.assign(is_fail=df["status"] == "FAIL")
      .groupby("endpoint")
      .agg(
          avg_latency=("latency_ms", "mean"),
          error_rate=("is_fail", "mean"),
          request_count=("endpoint", "count")
      )
)

endpoint_risk

endpoint_risk.sort_values(
    by=["error_rate", "avg_latency"],
    ascending=False
)

summary = []

for endpoint, row in endpoint_risk.iterrows():
    summary.append(
        f"{endpoint}: avg_latency={row.avg_latency:.0f}ms, "
        f"error_rate={row.error_rate*100:.1f}%, "
        f"requests={row.request_count}"
    )

summary

endpoint_risk.to_csv("day2_endpoint_risk_report.csv")